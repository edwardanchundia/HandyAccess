# HandyAccess

This project was made at the Coalition for Queens weekend Hackathon. Collaborating with three other people, we made use of the AV Foundation framework and Speech recognizing  framework to create an application useful for people that are visually impaired. At launch of the app, the user selects whether the person need hearing or visual assistance. They are then prompted to go through the different borough and resources they would like to access. Using the NYC Open Data API, we were able to locate landmarks or locations for the users needs. This all can be done visually or through siri speaking. 

<img src="https://github.com/edwardanchundia/HandyAccess/blob/master/Demo/HandyAccess_.gif?raw=true" width="320" />

The user is also able to find different places that are wheelchair accessible within their location. 

<img src="https://github.com/edwardanchundia/HandyAccess/blob/master/Demo/HandyAccess_2.gif?raw=true" width="320" />
