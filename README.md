# HandyAccess

This project was made at the Coalition for Queens weekend Hackathon. Collaborating with three other people, we made use of the AV Foundation framework and Speech recognizing framework to create an application useful for people that are visually impaired. At the launch of the app, the user selects whether the person needs hearing or visual assistance. They are then prompted to go through the different borough and resources they would like to access. Using the NYC Open Data API, we were able to locate landmarks or locations for the user's needs. This all can be done visually or through Siri speaking.

<img src="https://github.com/edwardanchundia/HandyAccess/blob/master/Demo/HandyAccess_.gif?raw=true" width="320" />

The user is also able to find different places that are wheelchair accessible within their location. 

<img src="https://github.com/edwardanchundia/HandyAccess/blob/master/Demo/HandyAccess_2.gif?raw=true" width="320" />
